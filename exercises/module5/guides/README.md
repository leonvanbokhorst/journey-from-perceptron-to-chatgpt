# Module 5: Sequence-to-Sequence Learning and Attention Mechanisms

This directory contains concept guides for understanding sequence-to-sequence learning and attention mechanisms.

## Contents

- **seq2seq_concept_guide.md**: Introduction to sequence-to-sequence architecture
- **attention_mechanisms.md**: Explanation of different attention mechanisms
- **transformer_components.md**: Deep dive into transformer components
- **nmt_applications.md**: Applications in neural machine translation

## Additional Resources

- Sequence to Sequence Learning with Neural Networks (Sutskever, 2014): [https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215)
- Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau, 2015): [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)
- Effective Approaches to Attention-based Neural Machine Translation (Luong, 2015): [https://arxiv.org/abs/1508.04025](https://arxiv.org/abs/1508.04025)
- Attention Is All You Need (Vaswani, 2017): [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
