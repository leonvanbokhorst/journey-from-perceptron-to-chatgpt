# Evolution of the Transformer: From the Perceptron to ChatGPT

## Module 1: The Perceptron and Early Neural Networks  

**Learning Objectives:**  

- Grasp the basic neuron model (perceptron) including inputs, weights, bias, and binary output.  
- Understand the concept of linear separability and why single-layer perceptrons can only solve linearly separable problems.  
- Learn the perceptron learning rule and implement a perceptron to classify simple data.  
- Recognize the historical significance of the perceptron (1950s) and the limitations that led to the search for multi-layer networks.

**Key Concepts Covered:**  

- *Perceptron Model:* The first artificial neuron model introduced by Frank Rosenblatt in 1958 ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=Frank%20Rosenblatt%20,and%20Wiesel%20was%20based%20on)). It computes a weighted sum of inputs and applies a step activation to produce a binary output (1 or 0).  
- *Linear Separability:* Perceptrons can classify data that are linearly separable in input space. They cannot solve problems like XOR which are not linearly separable (a limitation noted by Minsky & Papert in 1969) ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=Frank%20Rosenblatt%20,and%20Wiesel%20was%20based%20on)) ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=Some%20say%20that%20research%20stagnated,18)).  
- *Perceptron Learning Rule:* An iterative update rule (stochastic gradient descent on a simple error function) that adjusts weights to reduce classification errors. Guarantee of convergence exists if the data is linearly separable (Novikoff’s theorem).  
- *Historical Context:* The perceptron was an early breakthrough in machine learning ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=implementation%20of%20ANNs%20was%20by,2)), but its limitations (proved in *Perceptrons* (1969)) led to reduced funding and an “AI winter” in the 1970s ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=implementation%20of%20ANNs%20was%20by,2)) ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=Some%20say%20that%20research%20stagnated,18)). The idea of multi-layer perceptrons was known (Rosenblatt even described a multi-layer circuit) but there was no efficient training algorithm for multiple layers at that time ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=Frank%20Rosenblatt%20,and%20Wiesel%20was%20based%20on)).

**Core Readings:**  

- **Rosenblatt (1958)** – *The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.* The seminal paper introducing the perceptron ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=1.%20,1037%2Fh0042519)). Explains the model and the perceptron convergence algorithm.  
- **Minsky & Papert (1969)** – *Perceptrons.* MIT Press. A critical analysis of the perceptron’s limitations (notably the inability to solve XOR), which highlighted the need for multi-layer networks.  
- **3Blue1Brown YouTube Series (2017)** – *But what is a Neural Network?* (Chapter 1). An excellent visual introduction to perceptrons and how they classify data. Demonstrates why a single layer fails on XOR (intuitive geometry of linear separation).  
- **Michael Nielsen (Neural Networks and Deep Learning, Ch.1)** – *Using Perceptrons to Recognize Handwritten Digits.* An open online book chapter that introduces perceptrons with a simple classification example, building up to the need for multiple layers.

**Practical Exercises (PyTorch-based):**  

- *Implement a Perceptron:* Write a simple PyTorch script (or even just NumPy) to implement a single-layer perceptron. Train it on a tiny dataset (e.g. boolean logic gates AND/OR) where data is linearly separable, and verify it learns the correct classification boundary.  
- *XOR Experiment:* Attempt to train the same perceptron on the XOR truth table. Observe its failure to converge to a correct solution, reinforcing the concept of linear inseparability. Then, as a teaser for Module 2, add a hidden layer (making it a 2-layer network) and show that the XOR problem becomes solvable.  
- *Visualization:* Plot the decision boundary of a trained perceptron on a 2D dataset. This will help students see how the perceptron partitions the input space with a single linear boundary. Using a simple dataset (like points in a plane) classify them and visualize the result.

**Optional Stretch Ideas:**  

- *Continuous Activations:* Replace the perceptron’s step function with a sigmoid or ReLU and treat it as a one-neuron neural network (i.e. logistic regression). Use PyTorch’s autograd to train it on a small dataset, illustrating how gradient descent works on a smooth approximation of the perceptron.  
- *Perceptron Convergence Proof:* For mathematically inclined students, explore a proof or simulation that the perceptron learning algorithm converges for linearly separable data (the Perceptron Convergence Theorem). This can deepen understanding of why the algorithm works.  
- *Historical Exploration:* Read about early neural network history beyond the perceptron – e.g. Hebb’s rule (1949) as a precursor to learning, or the work of Ivakhnenko (1967) on deep networks ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=Group%20method%20of%20data%20handling,21)). While not directly hands-on, this provides richer historical context on how ideas evolved before backpropagation was invented.

## Module 2: Multi-Layer Perceptrons and Backpropagation  

**Learning Objectives:**  

- Understand how adding hidden layer(s) (multi-layer perceptrons, MLPs) allows neural networks to solve more complex, non-linearly-separable tasks (e.g. XOR).  
- Learn the backpropagation algorithm as the key method for training multi-layer neural networks by efficiently computing gradients.  
- Implement a simple two-layer neural network in PyTorch and train it on a dataset, observing how it learns via backpropagation.  
- Recognize the significance of the 1980s “deep learning” revival – how the discovery and popularization of backpropagation in 1986 reignited neural network research ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=form%20of%20backpropagation%20was%20developed,31)).  
- Explore different activation functions (sigmoid, ReLU) and understand why non-linear activation in hidden layers is essential.

**Key Concepts Covered:**  

- *Multi-Layer Perceptron (MLP):* A network consisting of an input layer, one or more hidden layers with non-linear activation, and an output layer. Even a single hidden layer (with sufficient neurons) gives an MLP the capacity to approximate arbitrary functions, overcoming the perceptron’s limitations.  
- *Backpropagation:* The training algorithm for MLPs that efficiently computes the gradient of a loss function with respect to all weights in the network by propagating errors backwards through layers ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=form%20of%20backpropagation%20was%20developed,31)). Introduced in theory in the 1970s and dramatically **popularized by Rumelhart, Hinton & Williams (1986) ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=form%20of%20backpropagation%20was%20developed,31))**, it enabled training of networks with hidden layers. Backpropagation applies the chain rule of calculus to compute weight updates layer by layer.  
- *Activation Functions:* Non-linear functions (sigmoid, tanh, ReLU, etc.) applied at each neuron's output. Key to making multi-layer networks powerful – without them, multiple layers would collapse into an equivalent single linear layer. Students will encounter sigmoids (historically used in the ’80s/’90s) and ReLU (a later innovation that improves training of deeper nets).  
- *Universal Approximation:* A theoretical insight that an MLP with one hidden layer can approximate any continuous function on compact input domains (given enough neurons). This underscores why, in principle, multi-layer networks are so powerful.  
- *Historical Notes:* After backpropagation’s discovery, researchers could train deeper networks. Early successes included handwritten digit recognition (LeCun et al., 1989) using a 5-layer network ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=The%20first%20deep%20learning%20multilayer,the%20currently%20dominant%20training%20technique)). However, training very deep networks was still challenging due to limited compute and issues like vanishing gradients (foreshadowing Module 4). The period from 1986 onward saw a renaissance in neural network research often termed the early “deep learning” era.

**Core Readings:**  

- **Rumelhart, Hinton & Williams (1986)** – *Learning representations by back-propagating errors* ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=1986%29.%20%22Learning%20representations%20by%20back,1038%2F323533a0)). A landmark paper in *Nature* that showed how error backpropagation can train hidden layers, leading to successful multilayer neural networks. *(If the original paper is not easily accessible, students can find summaries in textbooks or online resources.)*  
- **Michael Nielsen (Neural Networks and Deep Learning, Ch.2)** – *How Backpropagation Works.* A gentle, code-friendly walkthrough of the backpropagation algorithm, explaining the calculus behind it and including Python snippets. This reading solidifies understanding by deriving the weight update rules.  
- **3Blue1Brown YouTube Series (2017)** – *Gradient descent, how neural networks learn* (Chapter 2). A visual explanation of backpropagation and gradient descent, using intuitive graphics. It builds on the perceptron introduction to show how we train multi-layer nets by gradually reducing error ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=Main%20article%3A%20Backpropagation)) ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=form%20of%20backpropagation%20was%20developed,31)).  
- **MIT Deep Learning Lecture (OpenCourseWare)** – *Lecture on Multilayer Perceptrons.* (Optional video) Covers MLP architecture and includes a demo of training a network on example data, highlighting the role of activation functions and the effect of layer depth.

**Practical Exercises (PyTorch-based):**  

- *Build and Train an MLP:* Using PyTorch, define a neural network with one hidden layer (e.g. a 2-layer MLP) to solve the XOR problem or classify a simple dataset (like points inside/outside a circle). Use a sigmoid or ReLU activation in the hidden layer. Train the network using gradient descent (you can use PyTorch’s built-in optimizer and autograd). Verify that it learns a correct decision boundary for XOR or the chosen dataset – something a single perceptron could not do.  
- *Experiment with Activations:* Train the same 2-layer network on a dataset using different activation functions for the hidden layer (sigmoid vs ReLU). Compare training speed and accuracy. Students will observe that sigmoid neurons can saturate (gradients become small when outputs are near 0 or 1), whereas ReLU often leads to faster convergence.  
- *Manual Backprop Calculation:* For a very small network (e.g. 2 inputs -> 1 hidden neuron -> 1 output neuron), perform a manual forward and backward pass on a single data point to compute weight updates by hand. Then verify these against PyTorch’s autograd. This exercise reinforces the mechanics of backpropagation and chain rule.  
- *Train on a Real Dataset:* If resources permit, use an MLP to classify a subset of MNIST (handwritten digits). For example, train a network with one or two hidden layers on 1000 examples of two chosen digits. This gives hands-on experience with a real-world dataset and shows that even a simple neural net can learn basic image recognition.

**Optional Stretch Ideas:**  

- *Deeper Networks:* Increase the number of hidden layers (e.g. a 3-4 layer network) and try to train on a dataset. Observe phenomena like vanishing gradients or difficulty in convergence. This can segue into discussing why training “deep” networks was hard until certain techniques (like better initialization, ReLU, etc.) were developed in the 2000s.  
- *Initialization Experiments:* Explore the impact of weight initialization on training. For instance, initialize weights to very small values vs larger values and see how it affects learning for an MLP. This helps students appreciate practical considerations that were learned over time for successful training.  
- *Historical Paper:* Read the 1989 paper by LeCun et al. on *Backpropagation applied to handwritten ZIP code recognition*. It’s an early success story of MLPs (with convolution) in pattern recognition. Students interested in vision can see how combining neural nets with domain knowledge (like local receptive fields) led to the first convolutional neural networks (not core to transformers, but important in deep learning history).  
- *Math Proofs:* For those inclined, delve into a proof of the Universal Approximation Theorem for neural networks. This can be challenging, but even reading about the theorem in a textbook can reinforce why having at least one hidden layer is so powerful.

## Module 3: Recurrent Neural Networks and Sequence Modeling  

**Learning Objectives:**  

- Recognize why feed-forward networks (perceptrons/MLPs) are inadequate for sequence data (e.g. sentences, time series) where order and temporal dependencies matter.  
- Understand the structure of a Recurrent Neural Network (RNN): how it maintains a hidden state that is passed from one time step to the next, enabling memory of previous inputs.  
- Implement a basic RNN for a simple sequence task in PyTorch (e.g. character-level text generation or sequence classification) and visualize how the hidden state evolves.  
- Appreciate the challenges of training RNNs, especially the vanishing/exploding gradient problems for long sequences, motivating the need for improved architectures (next module).  
- Learn about early sequence modeling successes and limitations (e.g. Elman networks in 1990, struggles with long-term dependencies identified in the 1990s).

**Key Concepts Covered:**  

- *Sequence Data and Temporal Dependence:* Many AI tasks involve sequences (text, speech, time-series sensor data). Unlike fixed-size inputs, sequences can be arbitrarily long and have important order. RNNs are designed to handle such data by processing one element at a time while retaining a *state* that carries information forward.  
- *Recurrent Neural Network (RNN) Architecture:* An RNN has a feedback loop: the output of the hidden layer at time *t-1* becomes an input to the hidden layer at time *t*. This can be visualized as the same network cell unrolled over time steps. At each step, the RNN cell takes the current input and the previous hidden state to produce a new hidden state (and possibly an output). This weight-sharing across time enables the network to build a memory of past inputs.  
- *Hidden State and Memory:* The hidden state in an RNN acts as an internal memory that captures information from earlier in the sequence. For example, when reading a sentence word by word, the hidden state can (in theory) encode context from prior words to inform the processing of later words.  
- *Backpropagation Through Time (BPTT):* Training RNNs involves unrolling the network through time and applying backpropagation across all time steps (BPTT). Errors are propagated backward from later outputs to earlier ones, assigning credit (or blame) to weights for sequence-wide effects. This reveals a difficulty: gradients can diminish or blow up exponentially as they propagate through many time steps (the *vanishing/exploding gradient* problem).  
- *Vanishing/Exploding Gradients:* In deep or long RNNs, gradients can become extremely small or large. Early research (Hochreiter, 1991) identified that standard RNNs struggle to learn long-term dependencies because the contributions of inputs from far back in time either vanish (fade to almost zero) or explode ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=)). This means naive RNNs might “forget” information after a certain number of steps, as it fails to propagate sufficient error signal from distant past inputs.  
- *Variants of RNNs:* Early forms of RNNs include the **Elman network (1990)** and **Jordan network (1986)** which introduced basic recurrent structures for cognitive simulations ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=such%20networks%20can%20be%20affected,indefinitely%20far%20in%20the%20past)). They were important in proving RNNs could learn short-term structure in sequences. However, it became clear that without modifications, RNNs had trouble with long-term structure.  
- *Example Applications:* RNNs started showing promise in the late 1980s and 1990s for tasks like phoneme recognition, simple language modeling, and toy sequence prediction problems. For instance, an RNN can be used for next-character prediction in a text string (learn a language model), or for classifying sequences (e.g. sentiment classification from text of a sentence).  
- *Link to Transformers:* Understanding RNNs sets the stage for why the field sought new architectures. The sequential processing of RNNs means you *cannot easily parallelize* sequence handling (you must process step 1 before step 2, etc.), which becomes a bottleneck. This lack of parallelism and the gradient issues hinted that alternative approaches (like attention-based models in later modules) might overcome these limitations ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=One%20problem%20with%20seq2seq%20models,This%20allowed%20parallel%20processing)).

**Core Readings:**  

- **Elman (1990)** – *Finding Structure in Time.* This classic paper introduced a simple recurrent network applied to language-like sequences, showing how an RNN can form memory traces of previous inputs. It’s historically important for demonstrating that RNNs can learn temporal structure.  
- **Bengio et al. (1994)** – *Learning long-term dependencies with gradient descent is difficult* (IEEE Transactions on Neural Networks). This paper analytically explains the vanishing gradient problem in RNNs, reinforcing why naive RNNs struggle with long sequences. It provides insight into the challenges that subsequent architectures (like LSTM) aim to solve.  
- **Andrej Karpathy’s Blog (2015)** – *The Unreasonable Effectiveness of Recurrent Neural Networks.*  ([Andrej Karpathy blog](http://karpathy.github.io/#:~:text=Andrej%20Karpathy%20blog%20May%2021%2C,that%20learn%20to%20write)) A highly accessible and fun article that shows what vanilla RNNs (and LSTMs) can do, by training character-level models to generate text in the style of Shakespeare, Wikipedia, etc. It provides intuitive explanations of how RNNs process sequences and includes sample code.  
- **Colah’s Blog – *Understanding RNNs*** (optional). An illustrated blog post that introduces RNNs and visualizes how information flows through an unrolled network. It’s a good primer before diving into LSTMs.  
- **Dive into Deep Learning (Zhang et al.) – Chapter on RNNs.** This is an open-source textbook with Jupyter notebooks. The RNN chapter introduces sequence modeling with examples and includes code (in PyTorch/MXNet) for training a char-level language model. It’s a practical complement to the conceptual readings.

**Practical Exercises (PyTorch-based):**  

- *Character-Level Text Generation:* Implement a character-level language model using an RNN. For example, take a small corpus (like a collection of nursery rhymes or a Sherlock Holmes story) and train a simple RNN to predict the next character given the previous ones. Use PyTorch’s `nn.RNN` or manually implement the recurrence. After training, have your model generate text by iteratively predicting new characters. Students can see the model gradually learn basic patterns (e.g. common letter sequences, simple words).  
- *Sequence Classification:* Build an RNN to classify sequences. A simple task could be: given a sequence of numbers, determine if it contains a certain pattern. For instance, feed sequences of 0/1 and have the RNN output 1 if “0110” appears as a substring. This exercise shows how an RNN can encode the presence of a pattern in its state. Monitor the hidden state to see if it activates upon seeing the target pattern.  
- *Visualizing Hidden States:* For a trained RNN, take a test sequence and track the hidden state vector at each time step. Plot some representation of it (e.g. using PCA to reduce dimensionality) to observe how the state changes as it “remembers” or “forgets” information. For example, feed a sentence word by word and see if the hidden state at the end encodes sentiment (in a sentiment analysis task).  
- *Gradient Exploration:* Perform an experiment to illustrate vanishing gradients. Train an RNN on a very long sequence task (where the target at the end depends on an input from far back). Monitor the norm of gradients at each time step during backpropagation. Students may observe that gradients for early time steps become extremely small. This empirical demo reinforces the theoretical issue. (Tip: You can use a synthetic task, like sequence of 100 numbers where the goal is to predict the first number – vanilla RNN will struggle as length increases.)

**Optional Stretch Ideas:**  

- *Truncated BPTT:* Implement truncated backpropagation through time – a strategy where you cut the sequence into smaller chunks for training to mitigate very long-term dependencies. Have students see how this helps (or doesn’t) in training on long sequences. This was a common practical trick before more sophisticated solutions like LSTM were developed.  
- *Bidirectional RNNs:* Modify your RNN to be bidirectional (process the sequence from both start and end, concatenating two hidden states). Apply this to a task like name gender classification (predict if a name is male/female from its spelling – characters from both ends are informative). Students can compare performance to a unidirectional RNN. This exposes them to one way of improving context capture (used later in models like Bidirectional LSTMs and BERT).  
- *Explore Advanced RNN Variants:* Read about **Gated Recurrent Units (GRUs)** and other RNN variants (before formally covering them). Try swapping your vanilla RNN cell with PyTorch’s `nn.GRU` and see if it learns faster or handles longer sequences better on the same task. This can spark curiosity about how gating helps, leading into Module 4.  
- *Historical aside – Hopfield Networks:* For those interested in history, look up Hopfield networks (1982) which are another form of recurrent network (though different from sequence-processing RNNs). It’s not directly related to transformers, but it’s part of the lineage of “networks with memory”. It can be insightful to see the diverse ideas of “memory” in neural nets (Hopfield nets store patterns as attractors, whereas RNNs we study here store information in transient hidden states).

## Module 4: Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU)  

**Learning Objectives:**  

- Understand how LSTM networks extend RNNs with gating mechanisms to better preserve long-term information and mitigate the vanishing gradient problem.  
- Learn the components of the LSTM cell: input gate, forget gate, output gate, and cell state. Comprehend how these gates regulate information flow.  
- Implement or use LSTM/GRU layers in PyTorch for sequence tasks and compare their performance to vanilla RNNs, especially on tasks requiring long memory.  
- Recognize the historical development: LSTM introduced by Hochreiter & Schmidhuber (1997) ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=Long%20short,default%20choice%20for%20RNN%20architecture)) as a solution for long-term dependencies, later simplified into GRU (Cho et al., 2014).  
- Appreciate that LSTMs became the state-of-the-art choice for most sequence learning tasks in the late 2000s and 2010s (e.g. speech recognition, language modeling, translation pre-2017).

**Key Concepts Covered:**  

- *Vanishing Gradient Revisited:* In standard RNNs, information from far in the past decays exponentially. LSTMs address this by creating an explicit memory cell with linear interactions, so gradients can flow more directly over long time spans (less attenuation).  
- *LSTM Cell Structure:* Each LSTM unit has a **cell state** (c<sub>t</sub>) that runs through the cell with minor linear interactions, and a hidden state (h<sub>t</sub>) which is the output. Three gates modulate the cell:  
  - **Forget gate:** decides what information to throw away from the cell state. It takes the previous hidden state and current input and outputs a number between 0 and 1 for each element of the cell state (0 = “forget this completely”, 1 = “keep this”).  
  - **Input gate (and modulation):** decides what new information to store in the cell state. It has two parts: a gate that decides *if* to write to cell state, and a candidate value (often via a tanh layer) that could be written. The product of these is added to the cell state.  
  - **Output gate:** decides what part of the cell state to output (as the hidden state). Typically the cell state is passed through a tanh, and then filtered by the output gate to produce h<sub>t</sub>.  
  These gating mechanisms allow the network to keep long-term information in *c<sub>t</sub>* unabated (if the forget gate is near 1 and input gate near 0, the cell will carry information forward almost unchanged).  
- *GRU:* The Gated Recurrent Unit, introduced in 2014, is a streamlined variant of LSTM with only two gates (update and reset) combining some of the functions of LSTM’s three gates. GRUs lack a separate cell state – they operate directly on the hidden state. They often perform similarly to LSTMs on many tasks, with a simpler implementation.  
- *LSTM vs. Vanilla RNN:* LSTMs can learn problems with long gaps between relevant signals far better than basic RNNs. For example, an LSTM can be trained to remember a bit over 100 timesteps reliably, where a vanilla RNN would forget. In practice, LSTMs became the “default” RNN architecture for complex sequence tasks ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=Long%20short,default%20choice%20for%20RNN%20architecture)) due to this ability.  
- *Bidirectional LSTM and Stacking:* In practice, LSTM layers are often stacked (multi-layer LSTMs) and used bidirectionally for tasks like speech and text, to capture context from both directions. By the mid-2010s, many systems (Google’s speech recognition, etc.) were using deep LSTMs (e.g. 5-10 layers) with great success ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=match%20at%20L518%20Around%202006%2C,used%20in%20Google%20voice%20search)).  
- *Real-world Successes:* Highlight that by around 2015-2016, LSTMs achieved state-of-the-art in language modeling, machine translation, and speech. For instance, LSTMs were key in Google’s Neural Machine Translation system (replacing older statistical methods) and in Siri/voice assistants for speech-to-text ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=Around%202006%2C%20LSTM%20started%20to,used%20in%20Google%20voice%20search)) ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=LSTM%20broke%20records%20for%20improved,59)). This is the technology foundation just before transformers emerged.  
- *Limitations:* Even LSTMs aren’t perfect – they can still be slow to train for very long sequences due to sequential processing, and they have many parameters per cell. They handle moderate-term dependencies well, but extremely long sequences (hundreds of steps) or long-range dependencies with noise can still challenge them. These limitations set the stage for non-recurrent approaches (like attention mechanisms, next module). However, LSTMs were a huge leap forward, and understanding them is crucial to see what transformers generalize.

**Core Readings:**  

- **Hochreiter & Schmidhuber (1997)** – *“Long Short-Term Memory.”* Neural Computation. The original LSTM paper ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=Long%20short,default%20choice%20for%20RNN%20architecture)) ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=Around%202006%2C%20LSTM%20started%20to,used%20in%20Google%20voice%20search)). It is mathematically heavy, but the introduction explains the rationale and the structure of the LSTM. Students can skim to see the diagrams of the cell and gates.  
- **Olah’s Blog (2015)** – *Understanding LSTM Networks* ([Understanding LSTM Networks - Colah's Blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/#:~:text=A%20recurrent%20neural%20network%20can,a%20message%20to%20a%20successor)). A highly recommended visual tutorial. It steps through an LSTM cell diagram, explaining the purpose of each gate with color-coded illustrations. This blog will solidify the intuition for how LSTMs carry information.  
- **Chung et al. (2014)** – *Empirical Evaluation of Gated Recurrent Units (GRUs).* (Optional) This paper introduced GRUs and compared them to LSTMs on various tasks. Reading it is optional, but the first part describes the GRU architecture which students can contrast with LSTM.  
- **Illustrated Guide to LSTMs (video)** – There are several YouTube videos that animate the LSTM functioning (for example, *“LSTM explained”* by Edureka, or Deeplearning.ai’s videos on sequence models). Watching one can reinforce the gating concept through animation.  
- **Dive into Deep Learning – Sequence Models** (same book as prior module). The sections on LSTM and GRU include code examples. For instance, they show how to implement an LSTM from scratch and use it on a language model, which can be a valuable reference for coding exercises.

**Practical Exercises (PyTorch-based):**  

- *LSTM vs RNN on Text:* Revisit the char-level text generation task from Module 3. This time, use `nn.LSTM` in PyTorch (or `nn.GRU`) instead of a manual RNN. Train the model on the same dataset. Observe training speed or final performance differences. Students should find that the LSTM model converges to lower loss or maintains longer coherence in generated text than a vanilla RNN did. For example, an LSTM might produce more realistic word-like sequences or remember context (like keeping quotation marks paired).  
- *Remembering Long-Term Information:* Design a synthetic task to explicitly test long-term memory. For instance: input is a sequence of 20 random digits, except the first digit is special; the task is to output that first digit at the end of the sequence. Train a vanilla RNN and an LSTM/GRU to solve this. The vanilla RNN will struggle as the sequence grows, while the LSTM should learn to carry that first digit through. This exercise empirically demonstrates the LSTM’s ability to remember information over long gaps.  
- *PyTorch Sequence API:* Use PyTorch’s higher-level APIs for sequence data – for example, use `nn.LSTM` or `nn.GRU` layers and feed them a padded batch of sequences (utilizing PyTorch’s packing utilities). This gives practice in handling real sequence data where sequences have different lengths (common in NLP). You could use a dataset like IMDB movie reviews (treat each review as a sequence of words) and build a sentiment classifier with an LSTM. Even if using a subset of data for speed, this shows how LSTMs apply to language tasks.  
- *Implement a Gate:* For deeper understanding, implement one component of LSTM manually. For example, write a function that given an old cell state, old hidden state, and input, computes the new cell and hidden state using the LSTM formulas (you can use PyTorch operations for sigmoid, etc., but do the gating math “by hand” in code for one time-step). Verify that this yields the same result as PyTorch’s LSTM for one step. This low-level exercise cements the understanding of how data flows through the gates.

**Optional Stretch Ideas:**  

- *Implement a Full LSTM from Scratch:* Write your own LSTM class without using `nn.LSTM`, using basic matrix operations for gates. This is challenging but instructive. You can test it on a small sequence task and compare with the built-in version. This will demystify the “magic” by showing it’s just a combination of matrix multiplies and non-linearities.  
- *Tune the LSTM:* Explore tweaking hyperparameters that are unique to LSTMs. For example, experiment with the effect of sequence length on training. Or adjust the initialization of biases in the forget gate (some practitioners initialize the forget gate bias to a positive value like 1 or 2 to encourage remembering at the start). Does this impact learning speed? Such experiments connect theory to practical training heuristics.  
- *Read about Peephole Connections:* Some LSTM variants include peephole connections (the gates also get as input the previous cell state). Interested students can read Gers & Schmidhuber (2000) on LSTM improvements to see how slight modifications can affect performance on certain tasks.  
- *Explore Advanced Sequence Models:* By the late 2010s, researchers tried entirely different ways to handle sequences, e.g. **Temporal Convolutional Networks (TCNs)** or **Attention mechanisms (next module)**. Ambitious students can peek at a paper like “WaveNet” (Van den Oord et al. 2016, which used dilated convolutions for sequence generation) or *Neural Turing Machines* (Graves et al. 2014, which added an external memory). This isn’t necessary for the core curriculum, but it shows the landscape of ideas that ultimately converged with the advent of transformers.

## Module 5: Sequence-to-Sequence Learning and Attention Mechanisms  

**Learning Objectives:**  

- Understand the encoder–decoder (sequence-to-sequence) architecture for tasks that involve transforming one sequence into another (e.g. translating a sentence from one language to another).  
- Learn how the attention mechanism works and why it was introduced to overcome limitations of the basic seq2seq model (especially the bottleneck of compressing a long sequence into a single fixed-size vector).  
- Implement a simple sequence-to-sequence model with attention in PyTorch – for example, a small machine translation model – to see attention in action and appreciate its effect on model performance.  
- Trace the historical breakthroughs: Sutskever et al. (2014) introduced general seq2seq with LSTM, and Bahdanau et al. (2015) added attention to greatly improve translation quality ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=The%20idea%20of%20encoder,of%20%20344%20and%20Transformer)). These set the stage for the transformer by demonstrating the power of attention.  
- Recognize different types of attention (additive, dot-product, etc.) and the concept of *alignment* between sequences.

**Key Concepts Covered:**  

- *Sequence-to-Sequence (Seq2Seq) Architecture:* A framework with two components: an **encoder** that reads the input sequence and summarizes it into a context vector (typically the final hidden state), and a **decoder** that unfolds this context into an output sequence. For example, in translation, an encoder RNN reads an English sentence and produces a context vector; a decoder RNN then generates a French sentence from that vector. This architecture, using two coupled RNNs (or LSTMs), was a major advance for tasks with variable-length outputs ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=The%20idea%20of%20encoder,of%20%20344%20and%20Transformer)).  
- *Encoder Bottleneck:* In early seq2seq models, the entire information of the source sequence (regardless of length) had to be squeezed into the encoder’s final hidden state (a fixed-length vector). This creates a bottleneck: for long sentences, a single vector may not faithfully represent all the details, causing the decoder to produce inferior outputs for long or complex inputs. Researchers noticed translation quality dropped significantly on long sentences due to this limitation.  
- *Attention Mechanism:* Introduced by Bahdanau et al. (2015) to address the bottleneck. Instead of relying on one context vector, the decoder **attends** to the full sequence of encoder states at each decoding step. Concretely, at each output time step, the decoder computes attention weights (alignment scores) for each encoder hidden state, based on how relevant that part of the input is to the current decoding context. These weights (after softmax) sum to 1 and effectively focus the decoder on the parts of the input that are most useful for producing the next word. The decoder’s next output is then conditioned on a weighted sum of encoder states (often called the “context vector” for that step).  
- *How Attention is Computed:* In the original formulation (often called *additive attention*), the decoder’s previous hidden state is compared with each encoder hidden state via a small neural network to produce a score, and softmax is applied over scores to get attention weights. In practice, there are also simpler *dot-product attention* mechanisms where you take dot-products between decoder and encoder states to measure alignment. The key idea: the model learns to **align** bits of the input with bits of the output (e.g. in translation, aligning words that correspond in meaning) and this alignment happens dynamically for each output step ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=An%20image%20captioning%20model%20was,seq2seq%20model%20to%20image%20captioning)) ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=encoder,it%20encodes%20an%20input%20sequence)).  
- *Benefits of Attention:* The decoder no longer has to depend on a single vector for all time. It can retrieve relevant information from any part of the input sequence when needed. This dramatically improves performance on longer sequences and allows the model to handle more complexity. In machine translation, the attention-based model by Bahdanau et al. showed substantial gains and produced meaningful alignment matrices (which are interpretable – you can see which source words the model was focusing on for each translated word). Attention also reduces the burden on the encoder; the encoder can produce a richer sequence of representations without needing to compress everything.  
- *Visualizing Attention:* One can visualize the attention weights as a matrix for a given input-output pair (input length N by output length M). Bright spots indicate where the model attended strongly. These visualizations often show diagonal alignments in translation (word-by-word), but also shifts when word order differs, and help explain what the model has learned.  
- *Types of Attention:* Besides the basic global attention described, there are variations: *local attention* (where the model attends only to a window of positions around a guess), *self-attention* (a concept we will explore more in Module 6, where a sequence attends to itself), multi-head attention (Module 6). In seq2seq context, attention is typically *cross-attention* from decoder to encoder.  
- *Seq2Seq w/ Attention as Precursor to Transformers:* The success of attention in seq2seq suggested that paying selective focus is powerful. It also opened the door to eliminating the need for recurrence entirely, since one could directly relate positions in sequences. In fact, the Transformer can be seen as taking the attention idea to its limit: use attention everywhere and remove the sequential recurrence ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=One%20problem%20with%20seq2seq%20models,This%20allowed%20parallel%20processing)) ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=These%20strands%20of%20development%20were,the%20framework%20of%20Transformer%20architecture)). But before that leap, attention was used in conjunction with LSTMs.  
- *Other Applications of Attention:* Beyond translation, attention started being applied to image captioning (where an RNN decoder attends to parts of a CNN-encoded image) ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=An%20image%20captioning%20model%20was,seq2seq%20model%20to%20image%20captioning)), speech recognition, and more. It became a general module to improve sequence models by the late 2010s.

**Core Readings:**  

- **Sutskever et al. (2014)** – *Sequence to Sequence Learning with Neural Networks.* This is the paper from Google that introduced the basic encoder-decoder LSTM model for machine translation (English to French). It’s a milestone showing that purely neural machine translation could work, using two LSTMs. Skimming it is useful to understand the baseline seq2seq architecture (before attention).  
- **Bahdanau et al. (2015)** – *Neural Machine Translation by Jointly Learning to Align and Translate* ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=136.%20,10%29.%20%22Neural%20Turing)). This is the breakthrough paper that adds an attention mechanism to the seq2seq model. Core sections to read: how the alignment model works and the results showing improved translation quality. Figures in this paper illustrate the alignment (attention weights) between source and target words, which is helpful for intuition.  
- **Luong et al. (2015)** – *Effective Approaches to Attention-based Neural Machine Translation.* (Optional) This paper from Stanford compared different attention variants (e.g. dot-product vs. additive, global vs. local). It’s more technical, but if interested, it helps understand that there are many ways to implement attention, all sharing the same principle.  
- **The Illustrated Transformer (Sections on seq2seq vs Transformer)** – Before the transformer is introduced, this blog (Jay Alammar, 2018) spends a section explaining traditional seq2seq and attention mechanisms in an intuitive way ([The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/illustrated-transformer/#:~:text=In%20the%20previous%20post%2C%20we,try%20to%20break%20the%20model)) ([The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/illustrated-transformer/#:~:text=The%20Transformer%20was%20proposed%20in,knowledge%20of%20the%20subject%20matter)). It’s a good bridging resource.  
- **CS224n (Stanford) Lecture on Attention (2017)** – (Video/Slides) If available, this lecture from an NLP course covers the derivation of attention and shows examples. It reinforces the concept with visuals and was designed for students who had learned RNNs and were stepping up to attention.

**Practical Exercises (PyTorch-based):**  

- *Translation Model with Attention:* Implement a small-scale machine translation model using an encoder-decoder with attention. A feasible setup is to use a toy dataset (e.g., very short phrases in English and their translations in French or another language, or even a synthetic “translation” like mapping sequences of numbers to their sorted sequences). Utilize PyTorch’s `nn.LSTM` for encoder and decoder. Implement the attention as a separate module: at each decoder step, compute attention weights over encoder outputs and derive a context vector. You can follow the outline from PyTorch’s official seq2seq tutorial ([NLP From Scratch: Translation with a Sequence to Sequence Network and Attention — PyTorch Tutorials 2.6.0+cu124 documentation](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#:~:text=This%20is%20the%20third%20and,do%20our%20NLP%20modeling%20tasks)) ([NLP From Scratch: Translation with a Sequence to Sequence Network and Attention — PyTorch Tutorials 2.6.0+cu124 documentation](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#:~:text=In%20this%20project%20we%20will,translate%20from%20French%20to%20English)). Train this model and evaluate qualitatively: Does the attention mechanism help produce more accurate or fluent outputs compared to a baseline decoder without attention? Students can also inspect the learned attention weights for a few examples to see alignment (e.g., print or plot the weight matrix for a test sentence).  
- *Attention Visualization:* Using the model above, pick a test input-output pair and extract the attention weight matrix. Plot this matrix (rows = target words, columns = source words) as a heatmap. Label the axes with actual words if possible. This visual exercise helps confirm that the model is attending to sensible positions (e.g., when translating a noun, the highest weight might be on the corresponding noun in the source sentence).  
- *Comparative Experiment:* Train two seq2seq models on a task – one with attention and one without. For instance, a task could be reversing sequences of words or copying input to output with a delay. Compare their performance on long sequences that weren’t seen in training. Usually, the attention model will generalize better to longer sequences. This empirically demonstrates the advantage of attention in overcoming the fixed-context bottleneck.  
- *Implement Different Scoring Functions:* If the above tasks are done with a built-in or simple attention (say, dot-product), try implementing the alternative form (say, Bahdanau’s additive attention). Replace the attention calculation in your model and confirm you get similar or improved results. Although this is a bit advanced, it solidifies understanding that “attention” is a flexible mechanism (the specifics can vary).  
- *Apply Attention to a Different Domain:* As an extension, if time permits, apply the attention idea to image captioning. Use pre-extracted features of an image (e.g., a 8x8 grid of CNN features from an image) as the “encoder states” and an LSTM decoder to produce a caption, attending to different parts of the image at each word. Even with a very small image dataset, setting this up can broaden understanding (this is inspired by Xu et al. 2015). You might not train from scratch due to data needs, but you can use a pre-trained CNN for features and focus on the attention mechanism in the decoder.

**Optional Stretch Ideas:**  

- *Jointly Visualize Alignments and Outputs:* Write a utility to output, for a given input sentence, the translated output along with an alignment at each word. This could be done by printing the source word that had max attention weight for each generated target word. Evaluate qualitatively if the alignments make sense linguistically.  
- *Read Ahead – Self-Attention:* To transition to the next module, curious students can read about *self-attention* (sometimes called intra-attention). For example, read section 4 of the Transformer paper or related blogs where the idea of a sequence attending to itself (instead of an encoder attending to a separate sequence) is introduced ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=The%20idea%20of%20using%20attention,it%20encodes%20an%20input%20sequence)). Understanding self-attention in the context of an LSTM (as done in some research) will make the Transformer’s self-attention more intuitive.  
- *Combine with Convolution:* There were models like ConvS2S (Gehring et al. 2017) that used convolutional networks for seq2seq along with attention. If interested, see how those models structured the problem (they achieved parallelism with conv nets but still relied on attention for alignment). It’s useful to know that attention was not tied only to RNNs; it was a general concept applicable with different encoders/decoders.  
- *Implement Beam Search in Decoder:* In sequence generation tasks, one often uses beam search to find a better output sentence (since decoding greedily can miss the best translation). Implement a beam search decoder for your seq2seq model. This exercise isn’t directly about attention, but it’s a practical improvement often used in attention-based translation models to produce higher-quality output. It also cements understanding of how decoders generate sequences token by token.  
- *Explore Attention beyond Alignment:* Research like *Transformers* (to be covered next) generalizes attention. A stretch idea is to consider: could we *remove* the encoder hidden state passing entirely and rely purely on attention each time? (This is exactly what Transformers eventually did – using multiple attention “heads” and multiple layers.) Ponder this question or discuss as a group, to set the stage for the upcoming transformer architecture.

## Module 6: The Transformer Architecture – “Attention Is All You Need”  

**Learning Objectives:**  

- Understand the fundamental shift introduced by the Transformer (Vaswani et al., 2017): eliminating recurrence and convolutions entirely in favor of **self-attention** mechanisms and parallel processing of sequence positions ([[1706.03762] Attention Is All You Need](https://arxiv.org/abs/1706.03762#:~:text=,improving%20over%20the%20existing%20best)).  
- Learn the architecture of the Transformer in detail: multi-head self-attention, positional encoding, the structure of Transformer encoder and decoder blocks, and how they utilize attention (self-attention in encoders/decoders and cross-attention in decoders).  
- Comprehend why Transformers are faster to train than RNNs for long sequences (due to parallelism) and how they handle long-range dependencies effectively.  
- Implement a simplified Transformer or use PyTorch’s `nn.Transformer` module to experiment with the architecture on a small task (e.g. translation or a synthetic task), solidifying understanding of components like attention and feed-forward sublayers.  
- Recognize the significance of the Transformer’s results: it achieved state-of-the-art in translation with much less training cost, and it laid the groundwork for all modern large language models.  
- Familiarize with concepts like multi-head attention and positional encoding, which are key innovations enabling the Transformer to capture sequence order and multiple types of relations.

**Key Concepts Covered:**  

- *Self-Attention:* Unlike the seq2seq attention (which was between encoder and decoder), **self-attention** is an operation where a set of vectors (e.g. the sequence of representations in one layer) attends to each other within the same sequence. For each position *i*, you compute attention weights to all positions (including itself or excluding itself depending on context) to gather information from the whole sequence. In the Transformer, each word’s representation is updated by attending to all words in the previous layer, allowing direct information flow between any two positions, regardless of distance ([[1706.03762] Attention Is All You Need](https://arxiv.org/abs/1706.03762#:~:text=,improving%20over%20the%20existing%20best)).  
- *Scaled Dot-Product Attention:* The Transformer uses a particular kind of attention: for a query vector Q (from current position) and a set of key vectors K (all positions) with corresponding value vectors V, it computes attention outputs as:  
  \[ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^T}{\sqrt{d}}\right) V, \]  
  where $d$ is the dimensionality (for scaling). Here each position *i* in a sequence will use its query (derived from that position’s representation) to softly select weighted sums of all values (which are typically just the previous layer’s representations of each word). This is done in parallel for all positions using matrix operations.  
- *Multi-Head Attention:* Instead of performing a single attention, the Transformer uses multiple attention “heads” in parallel ([The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/illustrated-transformer/#:~:text=The%20encoders%20are%20all%20identical,layers)). Each head has its own projection of queries, keys, and values (usually smaller dimensional). They attend to the sequence and may focus on different aspects (e.g. one head might attend strongly to the next word, another to distant related words). The outputs of all heads are concatenated and linearly transformed to form the final output of the attention layer. Multi-head attention allows the model to capture different types of relationships simultaneously.  
- *Positional Encoding:* Since the model has no recurrence or convolution, it needs a way to inject information about the order of the sequence. Transformers add a positional encoding vector to the input embeddings at each position. These encodings are fixed patterns (like sine and cosine functions of different frequencies for each dimension) that provide unique signatures for each position ([The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/illustrated-transformer/#:~:text=The%20Transformer%20was%20proposed%20in,knowledge%20of%20the%20subject%20matter)). This way, even though self-attention is order-agnostic in computation, the model can learn to use these position signals to infer relative order or distance.  
- *Transformer Encoder:* The encoder is a stack of $N$ identical layers (e.g., $N=6$ in the original). Each layer has two sub-layers: (1) multi-head self-attention (each position attends to all positions in the previous layer’s output, or for the first layer, the input embeddings with positions) ([The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/illustrated-transformer/#:~:text=The%20encoders%20are%20all%20identical,layers)), and (2) a feed-forward network applied independently to each position (typically a 2-layer MLP that expands and then contracts dimensions). Around each sub-layer, there are skip connections (residuals) and layer normalization. The encoder processes the entire input sequence in parallel, refining representations through these attention+feedforward layers. The output of the final encoder layer is a sequence of vectors representing the input enriched with contextual information from the whole sequence.  
- *Transformer Decoder:* The decoder is also a stack of $N$ layers, with three sub-layers each: (1) multi-head self-attention (this time **masked** so that positions can’t see future tokens – ensuring autoregressive generation), (2) multi-head *cross-attention* where the decoder attends to the output of the encoder stack (this is analogous to the attention in seq2seq models, allowing the decoder to focus on relevant encoder states) ([The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/illustrated-transformer/#:~:text=The%20decoder%20has%20both%20those,attention%20does%20in%20seq2seq%20models)), and (3) a feed-forward network like the encoder. The decoder generates the output sequence one token at a time during inference, and the masking in self-attention ensures it can’t cheat by looking ahead. During training, the output sequence is shifted (so each position can only attend to earlier positions as “previous outputs”).  
- *Parallelism and Efficiency:* In the Transformer, because the encoder processes all positions simultaneously (self-attention can be computed with matrix operations over the whole sequence, and feed-forward is per position and parallelizable), training can be done with much more parallelization than an RNN which had to go step by step ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=Seq2seq%20models%20with%20attention%20still,140)) ([[1706.03762] Attention Is All You Need](https://arxiv.org/abs/1706.03762#:~:text=attention%20mechanism,GPUs%2C%20a%20small%20fraction%20of)). This means on GPUs/TPUs, Transformers can utilize full parallel computing over sequence length, making it much faster to train on long sequences. The original paper noted dramatic speed improvements – training their big model in days rather than weeks.  
- *Performance:* The paper *“Attention Is All You Need”* reported that the Transformer achieved better translation quality (in terms of BLEU score) than the existing best LSTM-based models, and did so with significantly less training cost ([[1706.03762] Attention Is All You Need](https://arxiv.org/abs/1706.03762#:~:text=attention%20mechanism,GPUs%2C%20a%20small%20fraction%20of)). It was quickly adopted across NLP tasks. The ability to handle long-range dependencies easily (because any position can attend to any other position in one step) gave it an edge in capturing context.  
- *Visualization of Attention Heads:* Just as with seq2seq attention, one can visualize attention in Transformers. With multi-head attention, one can examine what each head is focusing on. For example, in translation, some heads might learn to attend almost like alignment (similar to seq2seq attention), others might attend broadly to the surrounding words, etc. These visualizations in the original paper showed interpretable patterns (e.g. heads that tracked long-distance dependencies like matching punctuation or verb conjugation with subject).  
- *Inductive Bias:* Transformers have less inductive bias about sequence order than RNNs (which inherently have the concept of next/previous via recurrence). This is both a strength (more flexibility) and a consideration (positional encoding must be designed or learned). Empirically, the flexibility won out, as the model can learn whatever dependencies it needs.  
- *Evolution:* Mention that many refinements followed – e.g. better normalization, variations in positional encoding, etc. – but the core architecture from 2017 remains as the backbone of most cutting-edge models. BERT, GPT, T5, etc., are all based on the Transformer architecture or slight variants of it.

**Core Readings:**  

- **Vaswani et al. (2017)** – *Attention Is All You Need* ([[1706.03762] Attention Is All You Need](https://arxiv.org/abs/1706.03762#:~:text=,improving%20over%20the%20existing%20best)). The original Transformer paper. Key sections to focus on: the model architecture (Section 3) which describes multi-head attention and the encoder/decoder, and the results (Section 5) to see how it compared with previous models. Even if the math detail is dense, try to get the high-level ideas from the intro and diagrams.  
- **The Illustrated Transformer (Jay Alammar, 2018)** – *Blog post explaining the Transformer* ([The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/illustrated-transformer/#:~:text=In%20the%20previous%20post%2C%20we,try%20to%20break%20the%20model)) ([The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/illustrated-transformer/#:~:text=The%20encoders%20are%20all%20identical,layers)). This is an excellent resource that breaks down the architecture into digestible pieces with visuals. It goes through an example of how an input sentence’s words attend to each other in different heads, and how the encoding/decoding works. It’s highly recommended to go through this after reading the paper’s introduction.  
- **The Annotated Transformer (Harvard NLP)** – An online walkthrough of the Transformer code, with annotations explaining each part (available on Harvard NLP’s website) ([The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/illustrated-transformer/#:~:text=The%20Transformer%20was%20proposed%20in,knowledge%20of%20the%20subject%20matter)). This is great for those who want to see actual code (in PyTorch) of the model and training process, with commentary. Following this can be like a guided implementation experience.  
- **CS230/CS224n Lecture on Transformers** – Many deep learning courses updated their sequence modeling lectures to include Transformers after 2017. If slides or videos are available, they often provide a pedagogical perspective on the architecture and sometimes small examples.  
- **Original “Attention Is All You Need” Figure** – (Optional) The iconic diagram from the paper that shows the Transformer’s encoder and decoder stack with attention connections. Studying that figure while reading the blog explanation can help map the conceptual pieces to the actual architecture layout.  
- **BERT or GPT-2 papers (for context)** – Although detailed discussion is in Module 7, skimming the intro of *BERT: Pre-training of Deep Bidirectional Transformers* or *GPT-2: Language Models are Unsupervised Multitask Learners* can illustrate how quickly the Transformer was applied beyond translation. They will reference the Transformer architecture as the basis.

**Practical Exercises (PyTorch-based):**  

- *Build a Mini-Transformer:* Implement a small Transformer encoder for a simple task. For example, create a toy problem like: input is a sequence of numbers, output is the sequence sorted. A Transformer encoder-decoder could be used for this, but even an encoder that outputs a representation and a small decoder that outputs sorted numbers can test the concept. If implementing from scratch: start with one attention head and no encoder/decoder stack (just one layer) to ensure you get the idea working, then extend to multiple heads or layers. This is ambitious but extremely educational – even implementing the scaled dot-product attention and positional encoding manually will reinforce the concepts deeply.  
- *Use PyTorch’s `nn.Transformer`*: PyTorch provides a Transformer module. Use it to replicate one of your earlier tasks, such as translation or sequence prediction, but now with a Transformer. For example, use `nn.Transformer` to build a translator for the small dataset you used in Module 5. You will need to prepare input embeddings and positional encodings (PyTorch’s module often expects you to do that). Train the model and measure performance vs the LSTM+attention model. Students will experience that even a basic Transformer can achieve good results. This also provides familiarity with using pre-built components for complex models.  
- *Examine Attention Weights:* Hook into the model to retrieve attention weights from the multi-head attention layers. For a given input, inspect these weights. For simplicity, consider the first layer’s self-attention in the encoder. You can visualize or print which words each word attends to. Do you notice any pattern (like a word attending strongly to the next word, or perhaps to a particular token like end-of-sentence)? This helps in demystifying the “black box” – seeing that these heads often learn intuitive behaviors.  
- *Positional Encoding Effects:* Try an experiment where you *remove* or alter the positional encodings to see their necessity. For instance, if you zero-out the positional encoding vectors, train the model and see if it struggles to learn order-sensitive tasks (it should). Alternatively, experiment with learned positional embeddings vs. fixed sinusoidal ones (PyTorch’s transformer allows either). This will reinforce the importance of positional information in an architecture that is otherwise permutation-invariant.  
- *Efficiency Experiment:* If resources allow, compare training speed for a Transformer vs an LSTM on a reasonably long sequence task. For example, sequence length 50, batch size large – time a few epochs of training for both models. The Transformer should utilize parallelism better (especially on GPU). This can be a simple empirical validation of the theoretical speedup. Monitor also memory usage (Transformers use more memory as they have to store big attention matrices of size seq_len x seq_len, which is a known trade-off).  
- *Translate with Transformer (if dataset available):* Using a public small translation dataset (like the bilingual sentence pairs from the earlier seq2seq exercise or a subset of a corpus), train a Transformer-based translator. After training, have the model translate some sentences and compare with the LSTM+attention version’s translations. This not only tests the model but also gives insight into quality differences.  
- *Language Modeling:* Train a Transformer decoder (just the decoder part, no encoder) as a standalone language model on some text (similar to how GPT is just the decoder). You can use a small corpus (like Shakespeare sonnets or Linux kernel comments). Monitor the training perplexity and generate some text from it. This connects to how GPT models are trained (an autoregressive transformer on large text). Even on a small scale, students can see the Transformer learn character or word distributions.

**Optional Stretch Ideas:**  

- *Scaling Up Heads or Layers:* Experiment with increasing the number of heads or layers in your Transformer model (within reason for your compute). Does it improve performance on the task? Transformers often benefit from scaling width (heads, model dimension) and depth (layers). Observing this in a small setting can be interesting (though too few data might not show a huge difference).  
- *Probe Multi-Head Diversity:* Develop a metric or visualization to see if different attention heads are doing different things. For instance, compute the average attention weight matrix for each head across many samples and see if they differ or if certain heads specialize (maybe one consistently attends to the first token (like a CLS token), another has a broad spread, etc.). Research has shown heads sometimes have specializations; trying to detect that can be a mini-research project.  
- *Implement Transformer from Scratch:* Truly ambitious students can attempt a ground-up implementation of the Transformer (no using `nn.Transformer`). This involves implementing multi-head attention, positional encodings, and the whole encoder-decoder pipeline with residuals and layer norms. Testing it on a simple task (like identity mapping of a sequence) to ensure it learns can be a major accomplishment. This essentially follows the structure of the Harvard “Annotated Transformer” code.  
- *Understand Transformer Math Deeper:* Dive into the matrix algebra of why multi-head attention can be efficiently implemented. Perhaps derive the complexity: self-attention is O(n²) in sequence length, while an RNN is O(n) but not parallel – understanding this trade-off can be insightful. Also consider reading about recent efforts to improve Transformer efficiency (like Linformer, Reformer, etc.) that tackle the O(n²) issue – not in depth, but just to know that the community is actively building on this architecture.  
- *Beyond NLP:* Note that Transformers aren’t just for text. For a stretch, students can explore Vision Transformers (Dosovitskiy et al. 2020) where images are split into patches and treated like a sequence for a transformer to classify images ([Transformer (deep learning architecture) - Wikipedia](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#:~:text=Since%202020%2C%20Transformers%20have%20been,and%20relationships%20within%20the%20data)). Or *Transformer-based music generation* where sequences are musical notes. If resources permit, try a small experiment like using a Transformer to learn a simple music sequence pattern. This shows the versatility of the architecture.  
- *Read “Attention is All You Need” Subsequent Work:* The paper’s ideas were extended – e.g., *Relative Positional Encodings*, *Universal Transformers*, etc. An interesting short read is the introduction of the Transformer-XL (Dai et al. 2019) which allowed attention to span longer than a fixed length by propagating state. Or *BERT* (next module) which is essentially a transformer encoder trained in a specific way. Getting a head start on BERT or GPT by reading their model description (which will basically say “we use the Transformer architecture with these modifications”) can tie everything together.

## Module 7: Pre-trained Transformers and ChatGPT  

**Learning Objectives:**  

- Learn how the Transformer architecture enabled the era of **pre-trained language models** – models first trained on massive unsupervised data, then fine-tuned for specific tasks. Understand the difference between Transformer *encoder* models (like BERT) for understanding tasks and *decoder* models (like GPT) for generation.  
- Trace the development of large language models: from the initial GPT (2018) and BERT (2018) to scaling up with GPT-2 (2019) and GPT-3 (2020) ([Transformer (deep learning architecture) - Wikipedia](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#:~:text=In%20language%20modelling%2C%20ELMo%20,decoder%20model.%5B%2037)) ([Transformer (deep learning architecture) - Wikipedia](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#:~:text=Starting%20in%202018%2C%20the%20OpenAI,40)). Recognize key innovations like the masked language modeling objective and the capability of few-shot learning in GPT-3 ([](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf#:~:text=We%20demonstrate%20that%20scaling%20up,via%20text%20interaction%20with%20the)).  
- Understand at a high level what ChatGPT is: essentially an instance of GPT-3.5 (a large decoder-only Transformer with ~175B parameters) fine-tuned with human feedback to behave as an interactive chatbot ([
            ChatGPT, GPT-4, and Other Large Language Models: The Next Revolution for Clinical Microbiology? - PMC
  ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10640689/#:~:text=WHAT%20MAKES%20CHATGPT%20AND%20GPT,UNIQUE)). Grasp the concept of **Reinforcement Learning from Human Feedback (RLHF)** used to align the model’s outputs with human preferences.  
- Get hands-on experience using pre-trained transformer models with libraries like Hugging Face Transformers. Practice loading a pre-trained model and using it for inference (text generation or classification), and optionally fine-tuning a smaller pre-trained model on a new task.  
- Discuss the implications of large pre-trained models in AI engineering: how they changed the workflow (you can achieve state-of-the-art by fine-tuning a giant model instead of training from scratch), and the importance of prompt design/engineering when working with models like ChatGPT.

**Key Concepts Covered:**  

- *Pre-training and Fine-tuning Paradigm:* Modern NLP shifted to a paradigm where a model is first **pre-trained** on a generic task with abundant data (like predicting missing words in billions of sentences – a self-supervised task) and then **fine-tuned** on a specific task with comparatively little data. The Transformer made such large-scale pre-training feasible and effective ([Transformer (deep learning architecture) - Wikipedia](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#:~:text=Transformers%20typically%20are%20first%20pretrained,tuning%20commonly%20include)). This approach was analogous to how computer vision models were pre-trained on ImageNet and fine-tuned.  
- *BERT (2018):* Bidirectional Encoder Representations from Transformers. BERT used the Transformer encoder to build deep bidirectional representations by training on two tasks: (1) Masked Language Modeling – randomly masking some words in a sentence and training the model to predict them, and (2) Next Sentence Prediction – predicting if one sentence follows another. BERT’s model is an encoder stack that produces a rich vector for each token (and a special CLS token for the whole sequence). Fine-tuning BERT achieved state-of-the-art on many NLP tasks (like QA, sentiment, NER) with minimal task-specific architecture changes ([Transformer (deep learning architecture) - Wikipedia](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#:~:text=In%20language%20modelling%2C%20ELMo%20,decoder%20model.%5B%2037)). This was a huge validation of the pre-train/fine-tune approach in NLP.  
- *GPT series (2018 onward):* Generative Pre-trained Transformers by OpenAI. These are decoder-only Transformer models trained as language models (predict next token given previous tokens).
  - **GPT-1 (2018):** ~117M parameters, demonstrated that a single pre-trained Transformer (language model) fine-tuned on tasks like question answering or entailment could match earlier task-specific models.
  - **GPT-2 (2019):** ~1.5B parameters, trained on a very large corpus (WebText). It demonstrated surprisingly coherent text generation. OpenAI initially was cautious to release it fully, citing potential misuse. GPT-2 could generate paragraphs of text that were hard to distinguish from human-written in some cases, showing the power of scale in language modeling.
  - **GPT-3 (2020):** 175B parameters, an order of magnitude bigger. The key new insight was that extremely large models show **few-shot and zero-shot learning** abilities ([](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf#:~:text=We%20demonstrate%20that%20scaling%20up,via%20text%20interaction%20with%20the)). Instead of fine-tuning, GPT-3 could be prompted with a task description or a few examples and often perform the task without any weight update. This emergent capability meant you could get it to translate, answer questions, do basic arithmetic, all via cleverly designed prompts. GPT-3’s performance on many benchmarks was competitive with fine-tuned models, purely out of the box. The paper “Language Models are Few-Shot Learners” details this ([](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf#:~:text=of%02the,also%20identify%20some%20datasets%20where)). This shifted focus to leveraging prompts (prompt engineering) and scaling even further.  
- *Large Language Models (LLMs):* A term encompassing very large Transformer-based models (usually with billions of parameters) trained on broad data. These models, like GPT-3, can be adapted to a wide range of tasks. LLMs often contain a lot of implicit knowledge from their training data (e.g., facts, coding patterns, writing styles). However, they also can generate incorrect or unfiltered output since base training just focuses on next-word prediction.  
- *Fine-tuning vs Prompting:* Two ways to use these models. Fine-tuning involves updating the model’s weights on task-specific data (BERT fine-tuning approach). Prompting (with or without few-shot examples) involves crafting an input to coax the model to produce the desired output (GPT-3 style usage). The latter became popular due to the cost of fine-tuning huge models and the realization that a single model can do many tasks if prompted correctly.  
- *ChatGPT (2022):* An interactive chatbot based on the GPT-3 family. Specifically, ChatGPT is essentially *GPT-3.5* (an improved GPT-3, also called InstructGPT in some contexts) that has been fine-tuned with human demonstrations and preferences. OpenAI’s team collected example conversations where human AI trainers played both user and assistant to demonstrate ideal behavior, and they also gathered comparisons where humans ranked different model responses ([[2203.02155] Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155#:~:text=avenue%20for%20aligning%20language%20models,3)). Using this data, they first did supervised fine-tuning and then applied RLHF – the model generates multiple answers, a human (or a model trained to mimic a human) ranks them, and those rankings are used as a reward signal to further train the model using reinforcement learning (typically a policy gradient method). The result is a model that tries to be more **helpful, correct, and safe** in conversation ([[2203.02155] Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155#:~:text=using%20supervised%20learning,aligning%20language%20models%20with%20human)). For example, it will ask clarifying questions, refuse certain inappropriate requests, and attempt to ground its answers in facts it “believes”. ChatGPT was a breakthrough in usability – it could follow arbitrary instructions in a conversational format, far better than a raw GPT-3. Its public release led to mass adoption and awareness of AI capabilities ([Transformer (deep learning architecture) - Wikipedia](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#:~:text=Starting%20in%202018%2C%20the%20OpenAI,40)).  
- *InstructGPT and RLHF:* The paper “Training language models to follow instructions with human feedback” (Ouyang et al., 2022) details this fine-tuning of GPT-3 into a model that responds to instructions (this is essentially what ChatGPT is built on) ([[2203.02155] Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155#:~:text=avenue%20for%20aligning%20language%20models,3)). The key concept is alignment – aligning the model’s behavior with human intent and values. RLHF has become a standard approach to take a powerful but raw LLM and make it more aligned (less likely to produce harmful or nonsensical outputs).  
- *Use of Pre-trained Models in Practice:* For AI engineers, pre-trained models are now like APIs or libraries. You often start with a base model (e.g. “bert-base-uncased” or “gpt-2”) from a model zoo (like Hugging Face Hub) and either fine-tune it or prompt it for your task. This greatly lowers the barrier to building AI systems for specific tasks since you don’t need to train from scratch. We also discuss the importance of evaluating these models carefully because they may have biases or unexpected failure modes due to their training data.  
- *Examples of Fine-tuning:* Fine-tuning BERT for sentiment analysis (just add a classification head on the CLS token). Fine-tuning a GPT-2 on your company’s support chat data to make a specialized chatbot. Fine-tuning T5 (a text-to-text transformer) for summarization. These illustrate the versatility of the approach.  
- *Emergence of Chat Interfaces:* ChatGPT showed a new interaction paradigm: dialogue. Now, not only models answer single questions, but they maintain context over a conversation. The underlying technical feature is the model’s ability to take a very long prompt (the entire dialogue history) thanks to Transformers being able to handle long context windows. This is essentially feeding the conversation transcript as a prompt at each turn (truncated to fit model’s context length). It feels interactive and contextual to users, which was a big usability leap.  
- *Ethical and Practical Considerations:* Touch on the considerations of using such powerful models – e.g., the need for filtering to avoid inappropriate content, the cost of running large models (inference can be expensive), and deployment considerations (distilling models to smaller sizes for production, etc., as done in e.g. GPT-3 -> GPT-3.5 improvements). But details can be minimal, as focus is on technical learning.

**Core Readings:**  

- **Devlin et al. (2018)** – *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.* Read the introduction to understand the masked language modeling approach and how BERT is an encoder that can be fine-tuned for different tasks. It’s a key paper demonstrating the power of transformer encoders in NLP.  
- **Brown et al. (2020)** – *Language Models are Few-Shot Learners (GPT-3).* Skim this paper, especially the abstract and introduction ([](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf#:~:text=We%20demonstrate%20that%20scaling%20up,via%20text%20interaction%20with%20the)). It explains the scaling of model size and the surprising capability of prompting large models to do tasks without gradient updates. The paper’s Table 1 (if accessible) listing the 175 billion parameters and the tasks GPT-3 can do is insightful.  
- **Ouyang et al. (2022)** – *Training language models to follow instructions with human feedback (InstructGPT).* The abstract and conclusion give a sense of how they aligned GPT-3 with human preferences ([[2203.02155] Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155#:~:text=avenue%20for%20aligning%20language%20models,3)). This is essentially the basis of ChatGPT. The results showed that a 1.3B model fine-tuned with RLHF could outperform a 175B model on helpfulness ([[2203.02155] Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155#:~:text=reinforcement%20learning%20from%20human%20feedback,aligning%20language%20models%20with%20human)) – a striking result underlining the value of alignment.  
- **OpenAI Blog (Nov 30, 2022)** – *“ChatGPT: Optimizing Language Models for Dialogue.”* This blog post announced ChatGPT. It’s a non-technical summary of what ChatGPT is and how it was trained. It also lists some limitations and encourages feedback. Good for understanding the goals of the project (making the model refuse inappropriate requests, etc.). *(If the blog is unavailable, secondary sources summarizing ChatGPT’s development can be used.)*  
- **Hugging Face Transformers Course/Documentation** – Read the sections on how to use a pre-trained model for inference and how to fine-tune a model. Hugging Face provides an accessible interface to load models like BERT or GPT-2. This will directly help with the practical exercises.  
- **“GPT-4 System Card” or OpenAI technical report (2023)** – (Optional, for those interested in the very latest) Skimming OpenAI’s GPT-4 report can give a sense of how even larger models are evaluated and the effort in alignment. It’s beyond our scope, but it emphasizes that ChatGPT is not the end – models continue to improve.

**Practical Exercises (PyTorch/HuggingFace-based):**  

- *Sentiment Analysis with BERT:* Using Hugging Face Transformers, load a pre-trained BERT base model with a classification head (e.g., `bert-base-uncased` with a sequence classification head). Fine-tune it on a sentiment analysis dataset (like IMDB reviews or a smaller dataset for speed). This involves preparing the data, tokenizing text to BERT’s word-piece tokens, and training for a few epochs. Students will see that even a few epochs on a moderate dataset can yield high accuracy, thanks to BERT’s learned language understanding. This exercise demonstrates fine-tuning a transformer encoder for a downstream task.  
- *Text Generation with GPT-2:* Load a pre-trained `gpt2` model (117M version) and use it to generate text. You can use Hugging Face’s pipeline for text generation. Experiment with prompts: provide the model with different opening sentences and see how it continues. Try using the model in a few-shot manner: for example, prompt it with a couple of question-answer pairs and then a new question to see if it continues in that pattern (illustrating few-shot learning). Also play with generation parameters like temperature and top-k sampling to see their effect on the output (higher temperature = more random, etc.).  
- *Fine-tune GPT-2 (optional if resources):* Take the GPT-2 model and fine-tune it on a specific style of text – for example, all Shakespeare sonnets, or a set of tech blog articles. After fine-tuning (even on a small dataset), generate text and observe the style adaptation. This shows how a generative transformer can be specialized via fine-tuning. (Caution: GPT-2 fine-tuning can be slow on CPU, so this might be done on a subset or for a few steps just to illustrate the process.)  
- *Using ChatGPT via API:* (If students have access to OpenAI’s API or a free alternative) Write a small script that sends a prompt to a chat model (like OpenAI’s GPT-3.5-turbo or an open-source alternative like Alpaca or LLaMA-based chat model) and receives a response. Experiment with dialogue: maintain a conversation state and show how the model’s answers use context. While this isn’t exactly coding a model, it teaches how to interact with these models programmatically, which is a valuable engineering skill.  
- *Evaluate Model Outputs:* Take a few examples (could be from a validation set of a task, or some knowledge questions) and compare outputs from different models: e.g., your fine-tuned BERT classifier vs a logistic regression; or GPT-2 vs GPT-3 (if API accessible) on a short prompt. Discuss or note where the large model’s knowledge shows up or where it might hallucinate. This builds awareness of strengths/weaknesses of pre-trained models.  
- *Mask-Filling with BERT:* Use a pipeline for fill-mask with a model like `bert-base-uncased`. Give it a sentence with a `[MASK]` token (e.g., “Transformer models are trained by predicting missing [MASK] in text.”) and see what words BERT predicts for the mask. This exercise illustrates BERT’s native pre-training task and shows the kind of “knowledge” it has.  
- *DistilBERT or TinyGPT:* For practical deployment concerns, try using a smaller distilled model (like DistilBERT or DistilGPT-2) for one of the tasks above and compare its performance to the full model. This introduces the idea of model compression and that there are trade-offs between size and performance.  
- *Prompt Engineering Challenge:* Come up with a creative prompt to get a language model to do a non-trivial task, like write a short story about a topic in a certain style, or extract structured info from a paragraph. If using GPT-3 or ChatGPT (via an interface or API), test a few versions of your prompt to see which yields the best result. This can be a fun exercise showcasing that communicating with LLMs is almost like programming with natural language.

**Optional Stretch Ideas:**  

- *Train a Mini GPT:* Using a smaller dataset (like a few MB of text) and a smaller model, train a Transformer decoder from scratch as a language model. This is more of a deep learning engineering exercise – one can use libraries or even write a training loop. For instance, train a 6-layer Transformer on Shakespeare text. It won’t reach the prowess of GPT-2, but even seeing it learn basic English structure is rewarding. This reinforces how these large models are conceptually trained (just at scale).  
- *Explore Responsible AI Issues:* Examine outputs of your models for biases or errors. For example, use your fine-tuned sentiment BERT on some edge cases or biased sentences to see if it exhibits any unintended bias. Or prompt GPT-2 with sensitive topics and observe if it produces problematic content. Discuss why large pre-trained models can reflect societal biases present in training data and why companies put a lot of effort into moderating and aligning models like ChatGPT.  
- *Use a Retrieval-Augmented Model:* A growing trend is to combine transformers with retrieval (search) to ground their knowledge (e.g., RAG, OpenAI’s WebGPT). As a stretch, if data allows, try a simple version: when asked a factual question, instead of relying on the model’s parametric memory, first query a Wikipedia API or local knowledge base, then feed the retrieved text + question into the model. Compare answers with and without retrieval. This shows one way to mitigate the “hallucination” problem of language models by providing relevant context.  
- *Multi-Modality:* Transformers are being used in multi-modal models (e.g., text+image). If resources, students can explore a bit of CLIP (which uses a transformer to encode text and another to encode images, aligning them) or image generation (transformer-based models like DALL-E use transformers to encode image patches or as part of diffusion models). A simple exploration: use a tool to generate an image from a text prompt (like a smaller stable diffusion model) and discuss how transformer models (the text encoder in these systems) make it possible to connect language and vision.  
- *Continuous Learning with Transformers:* Investigate or experiment with how one might fine-tune a model like GPT-2 on new data without forgetting old capabilities (continual learning). For instance, fine-tune GPT-2 on a very different domain and see if it still can write in the old domain. This is advanced, but it brings up concepts of catastrophic forgetting and the need for techniques like adapter modules or LoRA (Low-Rank Adaptation) which allow updating a model for new tasks without altering the entire network weights.  
- *API Economy and Model Hubs:* Encourage exploring model hubs (like HuggingFace Hub) to see the variety of pre-trained models available (not just GPT/BERT, but others like T5, XLNet, RoBERTa, etc.). Maybe have students pick one model and report a unique thing about it (e.g., T5 treats every NLP problem as text-to-text, including classification). This broadens their perspective on the transformer landscape.  
- *Group Discussion*: Reflect on the journey from perceptron to ChatGPT. Have students articulate how each step addressed the shortcomings of the previous (e.g., perceptron -> MLP solved XOR, MLP -> RNN added sequence handling, RNN -> LSTM solved long-term memory, LSTM -> Attention solved bottleneck, Attention -> Transformer removed recurrence for parallelism, Transformer -> GPT scaled up to capture vast knowledge, GPT -> ChatGPT aligned that knowledge with human intent). This kind of big-picture synthesis solidifies the understanding of *why* each innovation mattered in the historical context ([History of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#:~:text=These%20strands%20of%20development%20were,the%20framework%20of%20Transformer%20architecture)) ([Transformer (deep learning architecture) - Wikipedia](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#:~:text=Starting%20in%202018%2C%20the%20OpenAI,40)).
